{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 00:04:38.534835: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 00:04:38.534862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 00:04:38.536365: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 00:04:38.540938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, r2_score\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GRU, Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(positions, years):\n",
    "    # Initialize an empty dictionary to store DataFrames by position\n",
    "    data_dict = {}\n",
    "    \n",
    "    for position in positions:\n",
    "        # Initialize an empty list to collect DataFrames for each year\n",
    "        df_list = []\n",
    "        \n",
    "        for year in years:\n",
    "            filepath = f'NFL-data-Players/{year}/{position}_season.csv'\n",
    "            df = pd.read_csv(filepath)\n",
    "            df['Year'] = year\n",
    "            df_list.append(df)\n",
    "        \n",
    "        # Concatenate all yearly DataFrames into a single DataFrame for the current position\n",
    "        position_data = pd.concat(df_list, ignore_index=True)\n",
    "        position_data = position_data.fillna(0)\n",
    "        \n",
    "        # Store the DataFrame in the dictionary with position as the key\n",
    "        data_dict[position] = position_data\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, player_id_col, target_col, feature_cols, timesteps=3):\n",
    "    # Sort data by player and year\n",
    "    data = data.sort_values(by=[player_id_col, 'Year'])\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    data[feature_cols] = scaler.fit_transform(data[feature_cols])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    unique_players = data[player_id_col].unique()\n",
    "    \n",
    "    for player_id in unique_players:\n",
    "        player_data = data[data[player_id_col] == player_id]\n",
    "        \n",
    "        for i in range(len(player_data) - timesteps):\n",
    "            sequence = player_data.iloc[i:i + timesteps][feature_cols].values\n",
    "            target = player_data.iloc[i + timesteps][target_col]\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            targets.append(target)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "# Example usage\n",
    "feature_columns = ['PassingYDS', 'PassingTD', 'RushingYDS', 'ReceivingYDS', 'etc...']  # Replace with your features\n",
    "X, y = prepare_sequences(data, 'PlayerId', 'TotalPoints', feature_columns, timesteps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, train_years, validation_years, test_year):\n",
    "    # Split data by year\n",
    "    train_data = data[data['Year'].isin(train_years)]\n",
    "    validation_data = data[data['Year'].isin(validation_years)]\n",
    "    test_data = data[data['Year'] == test_year]\n",
    "    \n",
    "    print(\"Training Data Shape:\", train_data.shape)\n",
    "    print(\"Validation Data Shape:\", validation_data.shape)\n",
    "    print(\"Test Data Shape:\", test_data.shape)\n",
    "    \n",
    "    # Prepare feature and target variables for each split\n",
    "    X_train = train_data.drop(['PlayerName', 'PlayerId', 'TotalPoints', 'Year'], axis=1)\n",
    "    y_train = train_data['TotalPoints']\n",
    "    X_val = validation_data.drop(['PlayerName', 'PlayerId', 'TotalPoints', 'Year'], axis=1)\n",
    "    y_val = validation_data['TotalPoints']\n",
    "    X_test = test_data.drop(['PlayerName', 'PlayerId', 'TotalPoints', 'Year'], axis=1)\n",
    "    y_test = test_data['TotalPoints']\n",
    "    \n",
    "    print(\"X_train Shape:\", X_train.shape)\n",
    "    print(\"X_val Shape:\", X_val.shape)\n",
    "    print(\"X_test Shape:\", X_test.shape)\n",
    "    \n",
    "    # Preserve identifiers for later use\n",
    "    train_ids = train_data[['PlayerName', 'PlayerId']]\n",
    "    val_ids = validation_data[['PlayerName', 'PlayerId']]\n",
    "    test_ids = test_data[['PlayerName', 'PlayerId']]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(GRU(units=64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1))  # Single output for the regression task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "# You can then compare these predictions with actual values\n",
    "for i in range(5):  # Display the first 5 predictions for example\n",
    "    print(f\"Actual: {y_test[i]}, Predicted: {predictions[i][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
