{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/player_stats.csv')\n",
    "df = df.sort_values(by=['player_id', 'season', 'week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all types of data in df\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "df = featureEngineering(df)\n",
    "\n",
    "qb_df_features, qb_df_target, rb_df_features, rb_df_target, wr_df_features, wr_df_target, te_df_features, te_df_target = cleanData(df)\n",
    "\n",
    "X_qb, y_qb = create_sequences(qb_df_features.values, qb_df_target.values, sequence_length)\n",
    "X_rb, y_rb = create_sequences(rb_df_features.values, rb_df_target.values, sequence_length)\n",
    "X_wr, y_wr = create_sequences(wr_df_features.values, wr_df_target.values, sequence_length)\n",
    "X_te, y_te = create_sequences(te_df_features.values, te_df_target.values, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_qb.shape\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred, key, split):\n",
    "    pl.figure(figsize=(10, 6))\n",
    "    pl.scatter(y_true, y_pred, alpha=0.3)\n",
    "    pl.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    pl.xlabel('Actual')\n",
    "    pl.ylabel('Predicted')\n",
    "    pl.title(f'Predictions vs. Actuals for {key.upper()} - Split {split + 1}')\n",
    "    pl.grid(True)\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'R² Score: {r2}')\n",
    "    return mse, mae, r2\n",
    "\n",
    "for key, predictions in rnn_predictions_gru.items():\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        dataset = datasets[key][i]\n",
    "        y_true = dataset['y_test']\n",
    "        y_pred = prediction\n",
    "\n",
    "        print(f\"Evaluating RNN model for {key} - split {i + 1}...\")\n",
    "        \n",
    "        # Plot Predictions vs. Actuals\n",
    "        plot_predictions(y_true, y_pred, key, i)\n",
    "\n",
    "        # Calculate and print evaluation metrics\n",
    "        mse, mae, r2 = evaluate_predictions(y_true, y_pred)\n",
    "\n",
    "        # Optionally, store the metrics for later comparison\n",
    "\n",
    "evaluation_results = {\n",
    "    'qb': [],\n",
    "    'rb': [],\n",
    "    'wr': [],\n",
    "    'te': []\n",
    "}\n",
    "\n",
    "for key, predictions in rnn_predictions_gru.items():\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        dataset = datasets[key][i]\n",
    "        y_true = dataset['y_test']\n",
    "        y_pred = prediction\n",
    "\n",
    "        mse, mae, r2 = evaluate_predictions(y_true, y_pred)\n",
    "        evaluation_results[key].append((mse, mae, r2))\n",
    "        \n",
    "pl.xlabel('Split Number')\n",
    "pl.ylabel('Mean Squared Error (MSE)')\n",
    "pl.title('MSE Across Splits for Each Position')\n",
    "pl.legend()\n",
    "pl.grid(True)\n",
    "pl.show()\n",
    "\n",
    "# Initialize the evaluation results dictionary\n",
    "evaluation_results = {\n",
    "    'qb': [],\n",
    "    'rb': [],\n",
    "    'wr': [],\n",
    "    'te': []\n",
    "}\n",
    "\n",
    "# Evaluate predictions and store the MSE, MAE, R² score\n",
    "for key, predictions in rnn_predictions_gru.items():\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        dataset = datasets[key][i]\n",
    "        y_true = dataset['y_test']\n",
    "        y_pred = prediction\n",
    "\n",
    "        mse, mae, r2 = evaluate_predictions(y_true, y_pred)\n",
    "        evaluation_results[key].append(mse)  # Storing only MSE for heatmap\n",
    "\n",
    "# Convert the evaluation results into a DataFrame for heatmap plotting\n",
    "import pandas as pd\n",
    "\n",
    "# Convert evaluation results to a DataFrame\n",
    "heatmap_data = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Plot the heatmap\n",
    "pl.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data.T, annot=True, cmap='coolwarm', fmt=\".3f\", cbar=True)\n",
    "\n",
    "pl.xlabel('Split Number')\n",
    "pl.ylabel('Position')\n",
    "pl.title('MSE Heatmap Across Splits and Positions')\n",
    "pl.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
